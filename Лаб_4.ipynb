{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX/Dykc37cjCtjopMGIan5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BVika/Machine_learning_methods/blob/main/%D0%9B%D0%B0%D0%B1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задания\n",
        "Реализовать GPT как в п.2"
      ],
      "metadata": {
        "id": "JnP4Vb-kbe-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка библиотек"
      ],
      "metadata": {
        "id": "cL7g_C46bNcv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJSgvGRua20z",
        "outputId": "a0f65afc-4f5a-42d1-956d-7ec420a3e80f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install pymorphy3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импорт Библиотек и Загрузка Данных"
      ],
      "metadata": {
        "id": "TgcaeVFIi21H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzk7jmlgdPR2",
        "outputId": "b033c426-9228-4004-b565-6111d5978c0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Текст(датабейс)"
      ],
      "metadata": {
        "id": "e_LgY0rYdSfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 'Солнце медленно поднималось над горизонтом, окрашивая небо в яркие оттенки оранжевого и розового. Птицы начали петь, наполняя утро мелодиями.'\n",
        "\n",
        "phrases = [\n",
        "    'Солнце поднималось',\n",
        "    'Небо окрашивалось',\n",
        "    'Птицы начали',\n",
        "    'Утро наполнилось',\n",
        "    'Горизонт был',\n",
        "    'Мир просыпался',\n",
        "    'Звуки природы',\n",
        "    'Время летело'\n",
        "    ]\n",
        "\n",
        "result = [\n",
        "    'горизонтом',\n",
        "    'в краски',\n",
        "    'петь',\n",
        "    'мелодиями',\n",
        "    'красивым',\n",
        "    'новыми',\n",
        "    'звучали',\n",
        "    'быстро'\n",
        "    ]"
      ],
      "metadata": {
        "id": "nyo7jgHKdS-G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fh_fRDkGmLU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "    def split_text(self, content: str) -> list[str]:\n",
        "        \"\"\"Токенизация текста.\"\"\"\n",
        "        return word_tokenize(content)\n",
        "\n",
        "    def normalize_words(self, tokens: list[str]) -> list[str]:\n",
        "        \"\"\"Лемматизация токенов.\"\"\"\n",
        "        morph = MorphAnalyzer()\n",
        "        return [morph.parse(word)[0].normal_form for word in tokens]\n",
        "\n",
        "    def reduce_words(self, tokens: list[str]) -> list[str]:\n",
        "        \"\"\"Стемминг токенов.\"\"\"\n",
        "        stemmer = SnowballStemmer(\"russian\")\n",
        "        return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    def create_vector(self, tokens: list[str]) -> list[int]:\n",
        "        \"\"\"Создание векторного представления слов.\"\"\"\n",
        "        vector_dict = {}\n",
        "        vector_result = []\n",
        "        for word in tokens:\n",
        "            if word not in vector_dict:\n",
        "                vector_dict[word] = len(vector_dict)\n",
        "            vector_result.append(vector_dict[word])\n",
        "        return vector_result\n",
        "\n",
        "    def filter_stopwords(self, tokens: list[str]) -> list[str]:\n",
        "        \"\"\"Удаление стоп-слов из токенов.\"\"\"\n",
        "        stop_words = set(stopwords.words('russian')).union(['.', ',', ':', '?', '!'])\n",
        "        return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    def word_frequency(self, tokens: list[str]) -> dict[str, int]:\n",
        "        \"\"\"Создание словаря частот слов.\"\"\"\n",
        "        freq_dict = {}\n",
        "        for word in tokens:\n",
        "            freq_dict[word] = freq_dict.get(word, 0) + 1\n",
        "        return freq_dict\n",
        "\n",
        "    def term_frequency(self, tokens: list[str]) -> dict[str, float]:\n",
        "        \"\"\"Расчет частоты термина (TF).\"\"\"\n",
        "        freq_dict = self.word_frequency(tokens)\n",
        "        total_tokens = len(tokens)\n",
        "        return {word: count / total_tokens for word, count in freq_dict.items()}\n",
        "\n",
        "    def inverse_document_frequency(self, documents: list[list[str]]) -> dict[str, float]:\n",
        "        \"\"\"Расчет обратной частоты документа (IDF).\"\"\"\n",
        "        idf_dict = {}\n",
        "        all_words = set(word for doc in documents for word in set(doc))\n",
        "        total_docs = len(documents)\n",
        "        for word in all_words:\n",
        "            idf_dict[word] = math.log(total_docs / sum(word in doc for doc in documents))\n",
        "        return idf_dict\n",
        "\n",
        "    def tf_idf(self, documents: list[list[str]], doc_index: int) -> dict[str, float]:\n",
        "        \"\"\"Расчет TF-IDF для заданного документа.\"\"\"\n",
        "        tf = self.term_frequency(documents[doc_index])\n",
        "        idf = self.inverse_document_frequency(documents)\n",
        "        return {word: tf[word] * idf[word] for word in tf}\n"
      ],
      "metadata": {
        "id": "dMYaQnC9yYs4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomTransformer:\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, learning_rate=0.01):\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Инициализация параметров модели\n",
        "        self.embeddings = np.random.randn(vocab_size, embed_size) * 0.01\n",
        "        self.W_query = np.random.randn(embed_size, hidden_size) * 0.01\n",
        "        self.W_key = np.random.randn(embed_size, hidden_size) * 0.01\n",
        "        self.W_value = np.random.randn(embed_size, hidden_size) * 0.01\n",
        "        self.W_output = np.random.randn(hidden_size, vocab_size) * 0.01\n",
        "\n",
        "\n",
        "    def softmax(self, x):\n",
        "        \"\"\"Применение функции softmax.\"\"\"\n",
        "        exp_x = np.exp(x - np.max(x))\n",
        "        return exp_x / np.sum(exp_x)\n",
        "\n",
        "    def forward_pass(self, input_indices):\n",
        "        \"\"\"Прямой проход через модель.\"\"\"\n",
        "        self.input_indices = input_indices\n",
        "        self.embedded_input = self.embeddings[input_indices]  # (seq_len, embed_size)\n",
        "\n",
        "        self.Q = self.embedded_input @ self.W_query  # (seq_len, hidden_size)\n",
        "        self.K = self.embedded_input @ self.W_key\n",
        "        self.V = self.embedded_input @ self.W_value\n",
        "        self.attention_scores = self.Q @ self.K.T / np.sqrt(self.embed_size)  # (seq_len, seq_len)\n",
        "        self.attention_weights = self.softmax(self.attention_scores)  # (seq_len, seq_len)\n",
        "\n",
        "        self.attended_values = self.attention_weights @ self.V  # (seq_len, hidden_size)\n",
        "        self.context_vector = np.mean(self.attended_values, axis=0)  # (hidden_size,)\n",
        "\n",
        "        self.logits = self.context_vector @ self.W_output  # (vocab_size,)\n",
        "        self.probabilities = self.softmax(self.logits)\n",
        "\n",
        "        return self.probabilities\n",
        "\n",
        "    def backward_pass(self, target_index):\n",
        "        \"\"\"Обратный проход для обновления весов.\"\"\"\n",
        "        d_logits = self.probabilities.copy()\n",
        "        d_logits[target_index] -= 1  # (vocab_size,)\n",
        "\n",
        "        # Расчет градиентов\n",
        "        dW_output = np.outer(self.context_vector, d_logits)  # (hidden_size, vocab_size)\n",
        "        d_context = d_logits @ self.W_output.T  # (hidden_size,)\n",
        "\n",
        "        d_attended_values = np.ones_like(self.attended_values) * d_context / self.attended_values.shape[0]  # (seq_len, hidden_size)\n",
        "        dV = self.attention_weights.T @ d_attended_values  # (seq_len, hidden_size)\n",
        "\n",
        "        d_attention_weights = d_attended_values @ self.V.T  # (seq_len, seq_len)\n",
        "        d_scores = d_attention_weights * self.attention_weights * (1 - self.attention_weights)  # (seq_len, seq_len)\n",
        "\n",
        "        dQ = d_scores @ self.K / np.sqrt(self.embed_size)\n",
        "        dK = d_scores.T @ self.Q / np.sqrt(self.embed_size)\n",
        "\n",
        "        # Градиенты для весов\n",
        "        dW_query = self.embedded_input.T @ dQ\n",
        "        dW_key = self.embedded_input.T @ dK\n",
        "        dW_value = self.embedded_input.T @ dV\n",
        "\n",
        "        # Градиенты для эмбеддингов\n",
        "        d_embedding = dQ @ self.W_query.T + dK @ self.W_key.T + dV @ self.W_value.T  # (seq_len, embed_size)\n",
        "\n",
        "        for i, idx in enumerate(self.input_indices):\n",
        "            self.embeddings[idx] -= self.learning_rate * d_embedding[i]\n",
        "\n",
        "        # Обновление весов\n",
        "        self.W_output -= self.learning_rate * dW_output\n",
        "        self.W_query -= self.learning_rate * dW_query\n",
        "        self.W_key -= self.learning_rate * dW_key\n",
        "        self.W_value -= self.learning_rate * dW_value\n",
        "\n",
        "    def train_step(self, input_indices, target_index):\n",
        "        \"\"\"Шаг обучения.\"\"\"\n",
        "        self.forward_pass(input_indices)\n",
        "        self.backward_pass(target_index)\n",
        "\n",
        "    def predict(self, input_indices):\n",
        "        \"\"\"Предсказание на основе входных индексов.\"\"\"\n",
        "        probabilities = self.forward_pass(input_indices)\n",
        "        return np.argmax(probabilities)\n"
      ],
      "metadata": {
        "id": "yPkAAD1TG_8W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обработка текста и подготовка данных\n",
        "text_processor = TextProcessor()\n"
      ],
      "metadata": {
        "id": "WCNLRBlGHVTy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обработка текстов и создание векторов\n",
        "tokens = text_processor.filter_stopwords(text_processor.normalize_words(text_processor.split_text(input_text)))\n",
        "\n",
        "# Векторизация словаря\n",
        "vocabulary = list(set(tokens))\n",
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n"
      ],
      "metadata": {
        "id": "XUedFTpBHVed"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовка данных для обучения\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for phrase, label in zip(phrases, result):\n",
        "    print(f\"Обрабатываемая фраза: {phrase}, метка: {label}\")  # Выводим фразы и метки\n",
        "    tokens = text_processor.filter_stopwords(text_processor.normalize_words(text_processor.split_text(phrase)))\n",
        "    print(f\"Токены после фильтрации: {tokens}\")  # Выводим токены после фильтрации\n",
        "\n",
        "    if len(tokens) < 2:\n",
        "        print(\"Пропускаем фразу из-за недостаточной длины.\")  # Сообщение о пропуске\n",
        "        continue  # Пропускаем фразы с недостаточным количеством токенов\n",
        "\n",
        "    input_indices = [word_to_index.get(word, -1) for word in tokens if word in word_to_index]\n",
        "    input_indices = [index for index in input_indices if index != -1]  # Удаляем недопустимые индексы\n",
        "\n",
        "    if len(input_indices) == 0:\n",
        "        print(\"Пропускаем фразу из-за отсутствия допустимых токенов.\")  # Сообщение о пропуске\n",
        "        continue  # Пропускаем пустые массивы\n",
        "\n",
        "    label_index = word_to_index.get(label, -1)\n",
        "    if label_index == -1:\n",
        "        print(\"Пропускаем фразу из-за недопустимой метки.\")  # Сообщение о пропуске\n",
        "        continue  # Пропускаем, если метка недопустима\n",
        "\n",
        "    X.append(input_indices)\n",
        "    y.append(label_index)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYCyOTG9HVgv",
        "outputId": "099bd247-b6cf-4dfc-b9b1-467b7993d5c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обрабатываемая фраза: Солнце поднималось, метка: горизонтом\n",
            "Токены после фильтрации: ['солнце', 'подниматься']\n",
            "Пропускаем фразу из-за недопустимой метки.\n",
            "Обрабатываемая фраза: Небо окрашивалось, метка: в краски\n",
            "Токены после фильтрации: ['небо', 'окрашиваться']\n",
            "Пропускаем фразу из-за недопустимой метки.\n",
            "Обрабатываемая фраза: Птицы начали, метка: петь\n",
            "Токены после фильтрации: ['птица', 'начать']\n",
            "Обрабатываемая фраза: Утро наполнилось, метка: мелодиями\n",
            "Токены после фильтрации: ['утро', 'наполниться']\n",
            "Пропускаем фразу из-за недопустимой метки.\n",
            "Обрабатываемая фраза: Горизонт был, метка: красивым\n",
            "Токены после фильтрации: ['горизонт']\n",
            "Пропускаем фразу из-за недостаточной длины.\n",
            "Обрабатываемая фраза: Мир просыпался, метка: новыми\n",
            "Токены после фильтрации: ['мир', 'просыпаться']\n",
            "Пропускаем фразу из-за отсутствия допустимых токенов.\n",
            "Обрабатываемая фраза: Звуки природы, метка: звучали\n",
            "Токены после фильтрации: ['звук', 'природа']\n",
            "Пропускаем фразу из-за отсутствия допустимых токенов.\n",
            "Обрабатываемая фраза: Время летело, метка: быстро\n",
            "Токены после фильтрации: ['время', 'лететь']\n",
            "Пропускаем фразу из-за отсутствия допустимых токенов.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка размеров\n",
        "print(f\"Количество примеров: {len(X)}, X: {X}, y: {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jk7rCv2Kzsf",
        "outputId": "9f30f4d0-d3f9-4875-b98e-1bc06f34654b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество примеров: 1, X: [[1, 11]], y: [15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка на пустые массивы перед обучением\n",
        "if len(X) == 0 or len(y) == 0:\n",
        "    print(\"Ошибка: нет данных для обучения. Проверьте входные данные.\")\n",
        "else:\n",
        "    # Обучение модели\n",
        "    model = CustomTransformer(vocab_size=len(vocabulary), embed_size=16, hidden_size=32)\n",
        "\n",
        "    for epoch in range(10000):\n",
        "        total_loss = 0\n",
        "        for i in range(len(X)):\n",
        "            model.train_step(X[i], y[i])\n",
        "            probs = model.forward_pass(X[i])\n",
        "            total_loss += -np.log(probs[y[i]] + 1e-9)\n",
        "        if epoch % 1000 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {total_loss / len(X)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKGGWYi_HVkD",
        "outputId": "3c51947f-224f-4e02-8e98-b66b42286f56"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.7725827325074905\n",
            "Epoch 1000, Loss: 2.7725672739214797\n",
            "Epoch 2000, Loss: 2.7725413257824734\n",
            "Epoch 3000, Loss: 2.7724869017732385\n",
            "Epoch 4000, Loss: 2.772344406388924\n",
            "Epoch 5000, Loss: 2.771822786448161\n",
            "Epoch 6000, Loss: 2.7679674992366503\n",
            "Epoch 7000, Loss: 2.123479707462627\n",
            "Epoch 8000, Loss: 0.006003902897369979\n",
            "Epoch 9000, Loss: 0.002327604353827304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Предсказание\n",
        "    print(\"\\nПредсказания для новых фраз:\")\n",
        "    new_phrases = [\n",
        "        'Солнце поднималось над горизонтом',\n",
        "        'Небо стало ярким',\n",
        "        'Птицы пели мелодии',\n",
        "        'Утро было прекрасным'\n",
        "    ]\n",
        "\n",
        "    for phrase in new_phrases:\n",
        "        print(f\"\\nОбрабатываемая новая фраза: '{phrase}'\")\n",
        "        tokens = text_processor.filter_stopwords(text_processor.normalize_words(text_processor.split_text(phrase)))\n",
        "        print(f\"Токены после фильтрации: {tokens}\")\n",
        "\n",
        "        input_indices = [word_to_index.get(word, -1) for word in tokens if word in word_to_index]\n",
        "        input_indices = [index for index in input_indices if index != -1]  # Удаляем недопустимые индексы\n",
        "\n",
        "        if len(input_indices) == 0:\n",
        "            print(\"Нет допустимых токенов для предсказания.\")\n",
        "            continue\n",
        "\n",
        "        predicted_index = model.predict(input_indices)\n",
        "        predicted_word = index_to_word.get(predicted_index, \"Неизвестное слово\")\n",
        "        print(f\"Предсказанное слово: '{predicted_word}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MtLjQQCHVmj",
        "outputId": "9f51f828-a320-4b4f-d30b-10a6166a3dd9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Предсказания для новых фраз:\n",
            "\n",
            "Обрабатываемая новая фраза: 'Солнце поднималось над горизонтом'\n",
            "Токены после фильтрации: ['солнце', 'подниматься', 'горизонт']\n",
            "Предсказанное слово: 'петь'\n",
            "\n",
            "Обрабатываемая новая фраза: 'Небо стало ярким'\n",
            "Токены после фильтрации: ['небо', 'стать', 'яркий']\n",
            "Предсказанное слово: 'петь'\n",
            "\n",
            "Обрабатываемая новая фраза: 'Птицы пели мелодии'\n",
            "Токены после фильтрации: ['птица', 'петь', 'мелодия']\n",
            "Предсказанное слово: 'петь'\n",
            "\n",
            "Обрабатываемая новая фраза: 'Утро было прекрасным'\n",
            "Токены после фильтрации: ['утро', 'прекрасный']\n",
            "Предсказанное слово: 'птица'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lzTYB_fUHVpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Ensure to download the necessary NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.vectorizer = None\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        tokens = nltk.word_tokenize(text.lower())\n",
        "        return [self.lemmatizer.lemmatize(token) for token in tokens if token.isalnum()]\n",
        "\n",
        "    def fit_vectorizer(self, corpus):\n",
        "        self.vectorizer = CountVectorizer()\n",
        "        self.vectorizer.fit(corpus)\n",
        "\n",
        "    def transform(self, text):\n",
        "        return self.vectorizer.transform([text]).toarray()\n",
        "\n",
        "class SingleHeadAttention:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.Wq = np.random.rand(input_dim, output_dim)\n",
        "        self.Wk = np.random.rand(input_dim, output_dim)\n",
        "        self.Wv = np.random.rand(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = x.dot(self.Wq)\n",
        "        K = x.dot(self.Wk)\n",
        "        V = x.dot(self.Wv)\n",
        "\n",
        "        scores = np.dot(Q, K.T) / np.sqrt(Q.shape[-1])\n",
        "        attention_weights = self.softmax(scores)\n",
        "        output = np.dot(attention_weights, V)\n",
        "        return output\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "class SimpleNeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.W1 = np.random.rand(input_size, hidden_size)\n",
        "        self.b1 = np.zeros(hidden_size)\n",
        "        self.W2 = np.random.rand(hidden_size, output_size)\n",
        "        self.b2 = np.zeros(output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.sigmoid(np.dot(x, self.W1) + self.b1)\n",
        "        output = self.sigmoid(np.dot(hidden, self.W2) + self.b2)\n",
        "        return output\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Example Usage\n",
        "text_processor = TextProcessor()\n",
        "corpus = [\"My name is Sasha\", \"Your name is John\", \"What is your name?\"]\n",
        "text_processor.fit_vectorizer(corpus)\n",
        "\n",
        "for sentence in corpus:\n",
        "    processed_sentence = text_processor.preprocess(sentence)\n",
        "    print(f\"Processed: {processed_sentence}\")\n",
        "    vectorized_sentence = text_processor.transform(sentence)\n",
        "    print(f\"Vectorized: {vectorized_sentence}\")\n",
        "\n",
        "# Example of using attention\n",
        "attention = SingleHeadAttention(input_dim=10, output_dim=5)\n",
        "x = np.random.rand(2, 10)  # Example input with batch size 2 and feature size 10\n",
        "attention_output = attention.forward(x)\n",
        "print(f\"Attention Output: {attention_output}\")\n",
        "\n",
        "# Simple Neural Network Example\n",
        "nn = SimpleNeuralNetwork(input_size=10, hidden_size=5, output_size=3)\n",
        "nn_output = nn.forward(x)\n",
        "print(f\"Neural Network Output: {nn_output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw3-ML5_B4Ob",
        "outputId": "f0a06438-3c14-4e9c-93d1-b5570f87b19f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed: ['my', 'name', 'is', 'sasha']\n",
            "Vectorized: [[1 0 1 1 1 0 0]]\n",
            "Processed: ['your', 'name', 'is', 'john']\n",
            "Vectorized: [[1 1 0 1 0 0 1]]\n",
            "Processed: ['what', 'is', 'your', 'name']\n",
            "Vectorized: [[1 0 0 1 0 1 1]]\n",
            "Attention Output: [[2.30215297 0.99614934 3.68678039 2.24913376 3.79272347]\n",
            " [2.30268349 0.99451035 3.68990265 2.25068694 3.8014551 ]]\n",
            "Neural Network Output: [[0.87960231 0.91520589 0.81668954]\n",
            " [0.8850876  0.9196608  0.82265472]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1OfhWoK1dOjH"
      }
    }
  ]
}